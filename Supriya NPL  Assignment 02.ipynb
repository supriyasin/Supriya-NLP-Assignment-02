{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b9228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What are Corpora?\n",
    "\n",
    "\"\"\"Corpora (singular: corpus) refer to large and structured sets of texts, often used for \n",
    "   linguistic research, natural language processing (NLP), and machine learning tasks. \n",
    "   These collections of texts can be diverse and encompass various languages, genres,\n",
    "   and styles. Corpora are crucial for studying language patterns, developing language\n",
    "   models, and training algorithms for tasks like text analysis, sentiment analysis, \n",
    "   machine translation, and more.\n",
    "\n",
    "   Corpora can be categorized into different types based on their content and purpose:\n",
    "\n",
    "   1. General Corpora: These include a broad range of texts from various sources, such as\n",
    "      newspapers, books, websites, and more. They are used for general linguistic analysis \n",
    "      and modeling.\n",
    "\n",
    "   2. Specialized Corpora: These focus on specific domains or topics, like medical texts,\n",
    "      legal documents, technical literature, etc. Specialized corpora are designed to cater\n",
    "      to the needs of particular research areas.\n",
    "\n",
    "   3. Parallel Corpora: These contain texts in multiple languages that are aligned sentence \n",
    "      by sentence. Parallel corpora are commonly used for machine translation and cross-linguistic \n",
    "      research.\n",
    "\n",
    "   4. Historical Corpora: These consist of texts from different historical periods, allowing \n",
    "      researchers to study language evolution and changes over time.\n",
    "\n",
    "   5. Spoken Corpora: Texts derived from spoken language, such as transcriptions of conversations \n",
    "      or recordings, are used to study spoken language patterns and for developing speech\n",
    "      recognition systems.\n",
    "\n",
    "   Corpora serve as the foundation for building and refining language models. Researchers use\n",
    "   them to analyze linguistic phenomena, extract patterns, and train algorithms to better \n",
    "   understand and generate human-like text.\"\"\"\n",
    "\n",
    "# 2. What are Tokens?\n",
    "\n",
    "\"\"\"In the context of natural language processing (NLP) and linguistics, a token refers to a\n",
    "   unit of text that has been extracted from a larger body of text. Tokens can be individual\n",
    "   words, subwords, or even characters, depending on the level of granularity at which text\n",
    "   is segmented. The process of breaking down a text into its constituent tokens is called tokenization.\n",
    "\n",
    "   Here are a few key points about tokens:\n",
    "\n",
    "   1. Word Tokens: In most cases, when people refer to tokens, they are talking about word tokens. \n",
    "      A word token is a single instance of a word in a text. For example, the sentence \"ChatGPT is \n",
    "      amazing\" consists of four word tokens: \"ChatGPT,\" \"is,\" \"amazing,\" and the punctuation mark \".\".\n",
    "\n",
    "   2. Subword Tokens: Some tokenization methods break words into smaller units called subword tokens. \n",
    "      This approach is often used in languages with complex morphology or in scenarios where dealing\n",
    "      with a large vocabulary is challenging. For instance, the word \"unbelievable\" might be tokenized \n",
    "      into \"un,\" \"believ,\" and \"able.\"\n",
    " \n",
    "   3. Character Tokens: At an even finer level, tokens can be individual characters. In this case,\n",
    "      each letter, space, or punctuation mark is considered a separate token.\n",
    "\n",
    "   Tokenization is a crucial preprocessing step in natural language processing tasks. It helps to\n",
    "   convert raw text into a format that can be easily processed by machine learning models. \n",
    "   Once tokenized, texts can be represented numerically, and models can learn patterns and \n",
    "   relationships between these tokens to perform various language-related tasks, such as text \n",
    "   classification, sentiment analysis, machine translation, and more.\"\"\"\n",
    "\n",
    "# 3. What are Unigrams, Bigrams, Trigrams?\n",
    "\n",
    "\"\"\"Unigrams, bigrams, and trigrams are different types of n-grams, which are contiguous sequences\n",
    "   of n items (or words) from a given sample of text or speech. These terms are commonly used in \n",
    "   natural language processing (NLP) and text analysis.\n",
    "\n",
    "   1. Unigrams (1-grams):\n",
    "      - Unigrams are individual words or tokens in a text.\n",
    "      - For example, in the sentence \"The cat is on the mat,\" the unigrams are: \n",
    "        \"The,\" \"cat,\" \"is,\" \"on,\" \"the,\" and \"mat.\"\n",
    "\n",
    "   2. Bigrams (2-grams):\n",
    "      - Bigrams are sequences of two consecutive words in a text.\n",
    "      - Using the same example sentence, the bigrams are: \"The cat,\" \"cat is,\" \"is on,\" \n",
    "        \"on the,\" and \"the mat.\"\n",
    "\n",
    "   3. Trigrams (3-grams):\n",
    "      - Trigrams are sequences of three consecutive words in a text.\n",
    "      - Continuing with the example sentence, the trigrams are: \"The cat is,\" \"cat is on,\" \n",
    "        \"is on the,\" and \"on the mat.\"\n",
    "\n",
    "   N-grams of higher orders (e.g., 4-grams, 5-grams) can also be used depending on the specific\n",
    "   requirements of the analysis. The choice of n-gram size depends on the context and the level \n",
    "   of detail desired in capturing patterns within the text.\n",
    "\n",
    "   N-grams are used in various NLP tasks, such as language modeling, text generation, and information \n",
    "   retrieval. They help in capturing local structures and dependencies within the text, allowing \n",
    "   algorithms to understand and generate more contextually relevant sequences of words.\"\"\"\n",
    "\n",
    "# 4. How to generate n-grams from text?\n",
    "\n",
    "\"\"\"Generating n-grams from text involves breaking down the text into contiguous sequences of\n",
    "   n items, where the items are typically words or characters. The process of creating n-grams\n",
    "   is known as \"tokenization\" and \"sliding window.\"\n",
    "\n",
    "   Here is a simple Python example using the NLTK (Natural Language Toolkit) library to \n",
    "   generate n-grams from a given text:\n",
    "\n",
    "   ```python\n",
    "   import nltk\n",
    "   from nltk import word_tokenize\n",
    "   from nltk.util import ngrams\n",
    "\n",
    "   def generate_ngrams(text, n):\n",
    "       # Tokenize the text into words\n",
    "       words = word_tokenize(text)\n",
    "\n",
    "       # Use the ngrams function to generate n-grams\n",
    "       ngrams_list = list(ngrams(words, n))\n",
    "\n",
    "       return ngrams_list\n",
    "\n",
    "   # Example text\n",
    "   text = \"This is an example sentence for generating n-grams.\"\n",
    "\n",
    "   # Generate bigrams (2-grams)\n",
    "   bigrams = generate_ngrams(text, 2)\n",
    "   print(\"Bigrams:\", bigrams)\n",
    "\n",
    "   # Generate trigrams (3-grams)\n",
    "   trigrams = generate_ngrams(text, 3)\n",
    "   print(\"Trigrams:\", trigrams)\n",
    "   ```\n",
    "\n",
    "   Make sure we have the NLTK library installed before running the code. We can install it using:\n",
    "\n",
    "   ```bash\n",
    "   pip install nltk\n",
    "   ```\n",
    "\n",
    "   In this example, the `word_tokenize` function is used to tokenize the input text into words, \n",
    "   and the `ngrams` function from NLTK is employed to generate n-grams of the specified order\n",
    "   (`n`). The resulting n-grams are then printed.\n",
    "\n",
    "   Note that we can adapt this example for character-level n-grams or adjust it for other\n",
    "   tokenization libraries if needed. The key is to tokenize the text into the appropriate \n",
    "   units (words, characters) and then create n-grams by sliding a window of size n over the \n",
    "   sequence of tokens.\"\"\"\n",
    "\n",
    "# 5. Explain Lemmatization\n",
    "\n",
    "\"\"\"Lemmatization is a natural language processing (NLP) technique that involves reducing words \n",
    "   to their base or root form, known as the \"lemma.\" The purpose of lemmatization is to normalize\n",
    "   words so that different grammatical forms or inflections of a word are treated as a single base form.\n",
    "\n",
    "    Here are the key points about lemmatization:\n",
    "\n",
    "   1. Lemma: The lemma of a word is its base or dictionary form. For example, the lemma of the\n",
    "      word \"running\" is \"run,\" and the lemma of \"better\" is \"good.\"\n",
    "\n",
    "   2. Inflections: Words in a language can have various inflected forms due to tense, number, \n",
    "      gender, or other grammatical features. Lemmatization reduces these inflected forms to a common base.\n",
    "\n",
    "   3. Lemmatizer: A lemmatizer is the algorithm or tool used for performing lemmatization. \n",
    "      It typically relies on linguistic rules and a database of word forms and their corresponding lemmas.\n",
    "\n",
    "   4. Part-of-Speech (POS) Information: Lemmatization often takes into account the part of speech\n",
    "      of a word because the base form may vary depending on whether the word is used as a noun, \n",
    "      verb, adjective, etc.\n",
    "\n",
    "   Here's an example using the NLTK library in Python:\n",
    "\n",
    "   ```python\n",
    "   from nltk.stem import WordNetLemmatizer\n",
    "   from nltk.tokenize import word_tokenize\n",
    "\n",
    "   # Example text\n",
    "   text = \"The cats are running in the garden.\"\n",
    "\n",
    "   # Tokenize the text\n",
    "   words = word_tokenize(text)\n",
    "\n",
    "   # Initialize the WordNet lemmatizer\n",
    "   lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "   # Lemmatize each word\n",
    "   lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "   print(\"Original words:\", words)\n",
    "   print(\"Lemmatized words:\", lemmatized_words)\n",
    "   ```\n",
    "\n",
    "   In this example, the NLTK library is used, and the WordNet lemmatizer is employed to lemmatize \n",
    "   each word in the given text. The output will show the original words and their corresponding \n",
    "   lemmatized forms.\n",
    "\n",
    "   Lemmatization is particularly useful in text preprocessing for tasks like information retrieval, \n",
    "   text mining, and machine learning, where having a consistent base form for words can simplify\n",
    "   analysis and improve the performance of language models.\"\"\"\n",
    "\n",
    "# 6. Explain Stemming\n",
    "\n",
    "\"\"\"Stemming is another text normalization technique in natural language processing (NLP), \n",
    "   but unlike lemmatization, stemming involves reducing words to their base or root form by\n",
    "   removing suffixes. The resulting stem may not always be a valid word in the language, \n",
    "   but it captures the core meaning of related words.\n",
    "\n",
    "   Here are the key points about stemming:\n",
    "\n",
    "   1. Stem: The stem is the root or base form of a word obtained after removing suffixes. \n",
    "      For example, the stem of \"running\" is \"run,\" and the stem of \"happiness\" is \"happi.\"\n",
    "\n",
    "   2. Suffix Stripping: Stemming algorithms use heuristics to remove common suffixes from words.\n",
    "      This process may result in stems that are not actual words but are effective for capturing\n",
    "      related meanings.\n",
    "\n",
    "   3. Speed: Stemming is generally faster than lemmatization because it relies on rule-based \n",
    "      methods rather than accessing a database of word forms.\n",
    "\n",
    "   4. Porter Stemmer and Snowball Stemmer: These are popular stemming algorithms in English. \n",
    "      The Porter Stemmer and Snowball Stemmer apply a series of rules to reduce words to their stems.\n",
    "\n",
    "   Here's an example using the NLTK library in Python with the Porter Stemmer:\n",
    "\n",
    "   ```python\n",
    "   from nltk.stem import PorterStemmer\n",
    "   from nltk.tokenize import word_tokenize\n",
    "\n",
    "   # Example text\n",
    "   text = \"The cats are running in the garden.\"\n",
    "\n",
    "   # Tokenize the text\n",
    "   words = word_tokenize(text)\n",
    "\n",
    "   # Initialize the Porter Stemmer\n",
    "   stemmer = PorterStemmer()\n",
    "\n",
    "   # Stem each word\n",
    "   stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "   print(\"Original words:\", words)\n",
    "   print(\"Stemmed words:\", stemmed_words)\n",
    "   ```\n",
    "\n",
    "   In this example, the NLTK library is used, and the Porter Stemmer is applied to stem each \n",
    "   word in the given text. The output will show the original words and their corresponding stemmed forms.\n",
    "\n",
    "   While stemming can be quick and effective for certain applications, it may result in stems \n",
    "   that are not always linguistically accurate or meaningful. Lemmatization, which considers\n",
    "   word meanings and parts of speech, is often preferred in scenarios where obtaining valid\n",
    "   words is crucial, such as in information retrieval or language understanding tasks.\"\"\"\n",
    "\n",
    "# 7. Explain Part-of-speech (POS) tagging\n",
    "\n",
    "\"\"\"Part-of-speech (POS) tagging is a natural language processing (NLP) task that involves assigning\n",
    "   a specific grammatical category (such as noun, verb, adjective, etc.) to each word in a sentence.\n",
    "   POS tagging is crucial for understanding the syntactic structure of a sentence and is used in \n",
    "   various NLP applications, including text analysis, machine translation, and information retrieval.\n",
    "\n",
    "   Key points about POS tagging:\n",
    "\n",
    "   1. POS Tags: Each word in a sentence is assigned a POS tag, which represents its grammatical\n",
    "      role or syntactic category. Common POS tags include:\n",
    "      - Noun (NN)\n",
    "      - Verb (VB)\n",
    "      - Adjective (JJ)\n",
    "      - Adverb (RB)\n",
    "      - Pronoun (PRP)\n",
    "      - Preposition (IN)\n",
    "      - Conjunction (CC)\n",
    "      - Determiner (DT)\n",
    "      - Interjection (UH)\n",
    "      - and more.\n",
    "\n",
    "   2. POS Tagging Models: POS tagging can be performed using rule-based approaches, statistical models,\n",
    "      or machine learning models. Machine learning models, especially those based on techniques like\n",
    "      Hidden Markov Models (HMMs) or more advanced methods like neural networks, have become popular\n",
    "      for POS tagging due to their ability to capture complex language patterns.\n",
    "\n",
    "   3. Ambiguity: POS tagging is challenging because words can have different meanings and functions\n",
    "      based on context. For example, the word \"lead\" can be a noun (e.g., \"a metal used in pipes\") \n",
    "      or a verb (e.g., \"to guide\").\n",
    "\n",
    "   Here's an example using the NLTK library in Python:\n",
    "\n",
    "   ```python\n",
    "   import nltk\n",
    "   from nltk import word_tokenize\n",
    "   nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "   # Example text\n",
    "   text = \"The cat is sitting on the mat.\"\n",
    "\n",
    "   # Tokenize the text\n",
    "   words = word_tokenize(text)\n",
    "\n",
    "   # Perform POS tagging\n",
    "   pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "   print(\"Original words:\", words)\n",
    "   print(\"POS tags:\", pos_tags)\n",
    "   ```\n",
    "\n",
    "   In this example, the NLTK library is used to tokenize the input text into words, and then the\n",
    "   `pos_tag` function is applied to assign POS tags to each word. The output will show the original\n",
    "   words along with their corresponding POS tags.\n",
    "\n",
    "   Accurate POS tagging is crucial for many downstream NLP tasks, as it provides information about\n",
    "   the syntactic structure of a sentence, which can be essential for understanding the meaning of \n",
    "   the text and extracting relevant information.\"\"\"\n",
    "\n",
    "# 8. Explain Chunking or shallow parsing\n",
    "\n",
    "\"\"\"Chunking, also known as shallow parsing, is a natural language processing (NLP) technique that\n",
    "   involves grouping words in a sentence into meaningful chunks or phrases based on their part-of-speech\n",
    "   (POS) tags. The goal of chunking is to identify and extract higher-level syntactic structures that \n",
    "   convey more information than individual words. These structures can include noun phrases, verb\n",
    "   phrases, and other grammatical constructs.\n",
    "\n",
    "   Key points about chunking:\n",
    "\n",
    "   1. Phrases: Instead of labeling each word with its POS tag, chunking identifies sequences of words\n",
    "      that form meaningful units or phrases. Common chunks include noun phrases (NP), verb phrases (VP), \n",
    "      prepositional phrases (PP), etc.\n",
    "\n",
    "   2. Chunking Patterns: Chunking often relies on patterns of POS tags to identify and extract chunks. \n",
    "      These patterns can be defined using regular expressions or other rule-based methods.\n",
    "\n",
    "   3. Example of Chunking: Consider the sentence \"The black cat is sitting on the mat.\" A chunking\n",
    "      analysis might identify the following chunks:\n",
    "      - Noun Phrase (NP): \"The black cat\"\n",
    "      - Verb Phrase (VP): \"is sitting\"\n",
    "      - Prepositional Phrase (PP): \"on the mat\"\n",
    "\n",
    "   Here's an example using the NLTK library in Python:\n",
    "\n",
    "   ```python\n",
    "   import nltk\n",
    "   from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "   nltk.download('punkt')\n",
    "   nltk.download('maxent_ne_chunker')\n",
    "   nltk.download('words')\n",
    "\n",
    "   # Example text\n",
    "   text = \"The black cat is sitting on the mat.\"\n",
    "\n",
    "   # Tokenize the text\n",
    "   words = word_tokenize(text)\n",
    "\n",
    "   # Perform POS tagging\n",
    "   pos_tags = pos_tag(words)\n",
    "\n",
    "   # Define a chunking pattern using regular expressions\n",
    "   chunking_pattern = r\"\"\"Chunk: {<DT>?<JJ>*<NN>}\"\"\"\n",
    "\n",
    "   # Create a chunk parser\n",
    "   chunk_parser = RegexpParser(chunking_pattern)\n",
    "\n",
    "   # Apply chunking\n",
    "   chunks = chunk_parser.parse(pos_tags)\n",
    "\n",
    "   # Display the result\n",
    "   print(chunks)\n",
    "   ```\n",
    "\n",
    "   In this example, the NLTK library is used to tokenize the input text, perform POS tagging, \n",
    "   and define a chunking pattern using regular expressions. The `RegexpParser` class is then \n",
    "   used to create a chunk parser, and the `parse` method is applied to identify chunks in the sentence.\n",
    "\n",
    "   Chunking is valuable for extracting structured information from text, and it serves as an \n",
    "   intermediate step in more complex NLP tasks, such as information extraction and named entity \n",
    "   recognition. It helps in capturing higher-level syntactic information beyond individual words.\"\"\"\n",
    "    \n",
    "#9. Explain Noun Phrase (NP) chunking\n",
    "\n",
    "\"\"\"Noun Phrase (NP) chunking is a specific type of chunking or shallow parsing that focuses on\n",
    "   identifying and extracting noun phrases from a sentence. A noun phrase is a group of words\n",
    "   centered around a noun that functions as a single unit within a sentence. NP chunking helps \n",
    "   in capturing and extracting meaningful information about entities and their attributes.\n",
    "\n",
    "   Key points about Noun Phrase (NP) chunking:\n",
    "\n",
    "   1. Definition of Noun Phrase (NP): A noun phrase is a syntactic construct that consists of a noun\n",
    "      and its modifiers (adjectives, determiners) as well as any associated words that complete the \n",
    "      meaning, such as prepositional phrases.\n",
    "\n",
    "   2. NP Chunking Patterns: NP chunking involves defining patterns based on part-of-speech (POS) \n",
    "      tags to identify and extract noun phrases. Common POS tags associated with nouns include DT \n",
    "      (determiner), JJ (adjective), and NN (noun).\n",
    "\n",
    "   3. Example of NP Chunking: Consider the sentence \"The black cat is sitting on the mat.\" An \n",
    "      NP chunking analysis might identify the following noun phrases:\n",
    "      - \"The black cat\"\n",
    "      - \"the mat\"\n",
    "\n",
    "   Here's an example using the NLTK library in Python:\n",
    "\n",
    "   ```python\n",
    "   import nltk\n",
    "   from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "   nltk.download('punkt')\n",
    "\n",
    "   # Example text\n",
    "   text = \"The black cat is sitting on the mat.\"\n",
    "\n",
    "   # Tokenize the text\n",
    "   words = word_tokenize(text)\n",
    "\n",
    "   # Perform POS tagging\n",
    "   pos_tags = pos_tag(words)\n",
    "\n",
    "   # Define an NP chunking pattern using regular expressions\n",
    "   np_chunking_pattern = r\"\"\"NP: {<DT>?<JJ>*<NN>}\"\"\"\n",
    "\n",
    "   # Create an NP chunk parser\n",
    "   np_chunk_parser = RegexpParser(np_chunking_pattern)\n",
    "\n",
    "   # Apply NP chunking\n",
    "   np_chunks = np_chunk_parser.parse(pos_tags)\n",
    "\n",
    "   # Display the result\n",
    "   print(np_chunks)\n",
    "   ```\n",
    "\n",
    "   In this example, the `np_chunking_pattern` is defined using regular expressions to capture \n",
    "   sequences of words that form noun phrases. The `RegexpParser` class is then used to create \n",
    "   an NP chunk parser, and the `parse` method is applied to identify and extract noun phrases \n",
    "   from the sentence.\n",
    "\n",
    "   NP chunking is useful for tasks such as named entity recognition, information extraction, and \n",
    "   text summarization, where identifying and understanding the noun phrases in a sentence can \n",
    "   provide valuable insights into the structure and content of the text.\"\"\"\n",
    "    \n",
    "# 10. Explain Named Entity Recognition\n",
    " \n",
    "\"\"\"Named Entity Recognition (NER) is a natural language processing (NLP) task that involves \n",
    "   identifying and classifying entities (such as names of persons, organizations, locations, \n",
    "   dates, and more) within a text. The goal of NER is to extract structured information about\n",
    "   specific entities and their types from unstructured text.\n",
    "\n",
    "   Key points about Named Entity Recognition (NER):\n",
    "\n",
    "   1. Entities: Entities are typically real-world objects that can be categorized into predefined\n",
    "      types. Common entity types include:\n",
    "   - Person: e.g., John Smith\n",
    "   - Organization: e.g., Google\n",
    "   - Location: e.g., New York City\n",
    "   - Date: e.g., January 1, 2022\n",
    "   - Time: e.g., 3:00 PM\n",
    "   - Money: e.g., $100\n",
    "   - Percent: e.g., 20%\n",
    "\n",
    "   2. Token-level and Entity-level Annotation: NER involves labeling each token (word or subword) \n",
    "      in a text with its corresponding entity type. The labeled entities can then be grouped together\n",
    "      to form complete named entities.\n",
    "\n",
    "   3. Supervised Learning: NER is often approached as a supervised learning task, where machine learning \n",
    "      models are trained on labeled datasets to recognize entities. Common models include Conditional \n",
    "      Random Fields (CRF), Support Vector Machines (SVM), and more recently, deep learning models like\n",
    "      Bidirectional Long Short-Term Memory networks (BiLSTM) and Transformer-based models.\n",
    "\n",
    "   4. Example of NER: Consider the sentence \"Apple Inc. was founded by Steve Jobs and Steve Wozniak\n",
    "      on April 1, 1976, in Cupertino, California.\" NER for this sentence might identify the following\n",
    "      named entities:\n",
    "   - ORGANIZATION: \"Apple Inc.\"\n",
    "   - PERSON: \"Steve Jobs,\" \"Steve Wozniak\"\n",
    "   - DATE: \"April 1, 1976\"\n",
    "   - LOCATION: \"Cupertino, California\"\n",
    "\n",
    "   Here's a simple example using the NLTK library in Python:\n",
    " \n",
    "   ```python\n",
    "   import nltk\n",
    "   from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "   nltk.download('punkt')\n",
    "   nltk.download('maxent_ne_chunker')\n",
    "   nltk.download('words')\n",
    "\n",
    "   # Example text\n",
    "   text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak on April 1, 1976, in Cupertino, California.\"\n",
    "\n",
    "   # Tokenize the text\n",
    "   words = word_tokenize(text)\n",
    "\n",
    "   # Perform POS tagging\n",
    "   pos_tags = pos_tag(words)\n",
    "\n",
    "   # Apply Named Entity Recognition using ne_chunk\n",
    "   ner_result = ne_chunk(pos_tags)\n",
    "\n",
    "   # Display the result\n",
    "   print(ner_result)\n",
    "   ```\n",
    "\n",
    "   In this example, the `ne_chunk` function from NLTK is used to perform named entity recognition. \n",
    "   The output is a tree structure where named entities are identified and labeled with their respective types.\n",
    "\n",
    "   Named Entity Recognition is an essential component in various NLP applications, including\n",
    "   information extraction, question answering, and sentiment analysis, as it enables the \n",
    "   extraction of structured information from unstructured text data.\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
